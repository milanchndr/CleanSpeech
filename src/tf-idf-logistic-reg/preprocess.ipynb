{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbb51a3",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data\n",
    "\n",
    "#### Purpose\n",
    "Before doing anything fancy, we just want to make sure the files exist, load correctly, and look right.  \n",
    "This helps us confirm:  \n",
    "1. The data paths and filenames are correct  \n",
    "2. The structure (columns, rows) is what we expect  \n",
    "3. The text content (`comment_text`) actually contains readable sentences  \n",
    "\n",
    "#### What to Observe\n",
    "- `shape`: how many rows and columns in each file  \n",
    "- Does the DataFrame show columns like `id`, `comment_text`, and labels?  \n",
    "- Do the comments look like proper sentences (not garbled text)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b3a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159571, 8)\n",
      "Test shape : (153164, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load and look at the data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./data\"   # since notebook is inside CleanSpeech/src/\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train_data.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test_data.csv\")\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape :\", test.shape)\n",
    "\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7593e",
   "metadata": {},
   "source": [
    "## 2. Basic Data Quality Checks\n",
    "\n",
    "#### Purpose\n",
    "Before we begin cleaning, it’s essential to understand the condition of our dataset.  \n",
    "This step helps us identify problems that might affect preprocessing or model training.  \n",
    "We’ll check three basic aspects:  \n",
    "1. Missing text values  \n",
    "2. Duplicate comments  \n",
    "3. Label balance (how common or rare each toxicity label is)\n",
    "\n",
    "#### What to Observe\n",
    "- Missing text rows indicate rows that should be dropped before modeling.  \n",
    "- Duplicate comments may bias the model and should be removed.  \n",
    "- Label proportions reveal if certain classes are rare or dominant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97eaf391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing text rows: 0\n",
      "Duplicate comments: 0\n",
      "\n",
      "Label counts:\n",
      "toxic            15294\n",
      "obscene           8449\n",
      "insult            7877\n",
      "severe_toxic      1595\n",
      "identity_hate     1405\n",
      "threat             478\n",
      "dtype: int64\n",
      "\n",
      "Label proportions (% of total):\n",
      "toxic            9.584\n",
      "obscene          5.295\n",
      "insult           4.936\n",
      "severe_toxic     1.000\n",
      "identity_hate    0.880\n",
      "threat           0.300\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Basic data quality checks\n",
    "\n",
    "TEXT = \"comment_text\"\n",
    "LABELS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# 1. Missing text values\n",
    "null_count = train[TEXT].isna().sum()\n",
    "print(\"Missing text rows:\", null_count)\n",
    "\n",
    "# 2. Duplicate comments\n",
    "dup_count = train.duplicated(subset=[TEXT]).sum()\n",
    "print(\"Duplicate comments:\", dup_count)\n",
    "\n",
    "# 3. Label balance\n",
    "if all(col in train.columns for col in LABELS):\n",
    "    label_sum = train[LABELS].sum().sort_values(ascending=False)\n",
    "    print(\"\\nLabel counts:\")\n",
    "    print(label_sum)\n",
    "    print(\"\\nLabel proportions (% of total):\")\n",
    "    print((label_sum / len(train) * 100).round(3))\n",
    "else:\n",
    "    print(\"\\nLabel columns not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fc3cc",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "\n",
    "#### Purpose\n",
    "After understanding the data and its quality, the next step is to prepare the text for modeling.  \n",
    "Raw comments often contain unwanted elements that can confuse the model.  \n",
    "Here we focus on cleaning and standardizing the text so that it’s easier for the model to learn meaningful patterns.  \n",
    "\n",
    "We will perform basic text preprocessing steps such as:  \n",
    "1. Lowercasing all text  \n",
    "2. Removing URLs and HTML tags  \n",
    "3. Removing extra spaces and line breaks  \n",
    "4. Optionally removing emojis and non-ASCII characters  \n",
    "\n",
    "These steps will help in reducing noise while keeping the useful information intact.\n",
    "\n",
    "#### What to Observe\n",
    "- Whether the cleaned text still conveys the same meaning as the original  \n",
    "- That URLs, HTML, and special characters have been successfully removed  \n",
    "- That no large chunks of text are accidentally deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad0275b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  explanation why the edits made under my userna...  \n",
       "1  d'aww! he matches this background colour i'm s...  \n",
       "2  hey man, i'm really not trying to edit war. it...  \n",
       "3  \" more i can't make any real suggestions on im...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Text Cleaning\n",
    "\n",
    "import re\n",
    "\n",
    "# Define cleaning patterns (same as found during EDA)\n",
    "url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "html_pattern = r\"<.*?>\"\n",
    "emoji_pattern = r\"[\\U00010000-\\U0010ffff]\"       # captures emojis and symbols\n",
    "non_ascii_pattern = r\"[^\\x00-\\x7F]+\"             # captures non-ASCII chars\n",
    "multi_space_pattern = r\"\\s+\"                     # normalize spaces\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_pattern, \" \", text)\n",
    "    text = re.sub(html_pattern, \" \", text)\n",
    "    text = re.sub(emoji_pattern, \" \", text)\n",
    "    text = re.sub(non_ascii_pattern, \" \", text)\n",
    "    text = re.sub(multi_space_pattern, \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to train and test sets\n",
    "train[\"clean_text\"] = train[\"comment_text\"].apply(clean_text)\n",
    "test[\"clean_text\"] = test[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "# Quick preview to ensure it worked\n",
    "train[[\"comment_text\", \"clean_text\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b2bbd",
   "metadata": {},
   "source": [
    "## 4. Final Cleanup and Train–Validation Split\n",
    "\n",
    "#### Purpose\n",
    "After cleaning the text, we need to ensure the dataset is consistent and ready for model training.  \n",
    "This step involves removing unnecessary rows and creating a proper split between training and validation data.  \n",
    "Doing this ensures that the model learns and is evaluated on distinct samples.\n",
    "\n",
    "We will perform the following actions:  \n",
    "1. Remove rows with missing or empty cleaned text  \n",
    "2. Remove duplicate comments to avoid repetition bias  \n",
    "3. Create a new column `any_toxic` to indicate whether a comment is toxic or not (useful for stratified split)  \n",
    "4. Split the data into training and validation sets in an 80–20 ratio  \n",
    "\n",
    "#### What to Observe\n",
    "- The number of rows before and after cleanup  \n",
    "- Whether `any_toxic` correctly represents toxic vs. non-toxic rows  \n",
    "- That the final train and validation sets have similar label proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd627ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 rows with missing or empty cleaned text.\n",
      "Removed 316 duplicate rows.\n",
      "Created 'any_toxic' column for binary toxic indicator.\n",
      "Final sizes:\n",
      "Train: (127397, 10)\n",
      "Validation: (31850, 10)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Final cleanup and train-validation split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Remove rows with missing or empty cleaned text\n",
    "before = len(train)\n",
    "train = train[train[\"clean_text\"].notna() & (train[\"clean_text\"].str.strip() != \"\")]\n",
    "after = len(train)\n",
    "print(f\"Removed {before - after} rows with missing or empty cleaned text.\")\n",
    "\n",
    "# 2. Remove duplicate comments (based on clean_text)\n",
    "before = len(train)\n",
    "train = train.drop_duplicates(subset=[\"clean_text\"])\n",
    "after = len(train)\n",
    "print(f\"Removed {before - after} duplicate rows.\")\n",
    "\n",
    "# 3. Create binary indicator for \"any_toxic\"\n",
    "LABELS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "if all(col in train.columns for col in LABELS):\n",
    "    train[\"any_toxic\"] = (train[LABELS].sum(axis=1) > 0).astype(int)\n",
    "    print(\"Created 'any_toxic' column for binary toxic indicator.\")\n",
    "else:\n",
    "    print(\"Label columns missing; skipped 'any_toxic' creation.\")\n",
    "\n",
    "# 4. Train–Validation split (80–20), stratified on \"any_toxic\"\n",
    "train_df, val_df = train_test_split(\n",
    "    train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[\"any_toxic\"] if \"any_toxic\" in train else None\n",
    ")\n",
    "\n",
    "print(\"Final sizes:\")\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Validation:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c764483",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "#### Purpose\n",
    "After completing cleaning and splitting, it’s good practice to save the processed datasets.  \n",
    "This allows future steps like feature extraction, modeling, and evaluation to load the clean data directly without repeating the preprocessing every time.\n",
    "\n",
    "We will save:  \n",
    "1. `clean_train.csv` — Cleaned training data  \n",
    "2. `clean_val.csv` — Validation data  \n",
    "3. `clean_test.csv` — Cleaned test data with no labels  \n",
    "\n",
    "#### What to Observe\n",
    "- Ensure the files are saved correctly in the `data/` folder  \n",
    "- Verify that the saved files contain the expected columns and number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4cb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned datasets:\n",
      "./data\\clean_train.csv\n",
      "./data\\clean_val.csv\n",
      "./data\\clean_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save cleaned datasets\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(DATA_DIR, \"clean_train.csv\")\n",
    "val_path   = os.path.join(DATA_DIR, \"clean_val.csv\")\n",
    "test_path  = os.path.join(DATA_DIR, \"clean_test.csv\")\n",
    "\n",
    "# Columns to include in outputs\n",
    "cols_to_save = [\"id\", \"comment_text\", \"clean_text\", \"any_toxic\"] + LABELS\n",
    "\n",
    "# Save train and validation sets\n",
    "train_df.to_csv(train_path, index=False, columns=[c for c in cols_to_save if c in train_df.columns])\n",
    "val_df.to_csv(val_path, index=False, columns=[c for c in cols_to_save if c in val_df.columns])\n",
    "\n",
    "# Test set has no labels\n",
    "test.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"Saved cleaned datasets:\")\n",
    "print(train_path)\n",
    "print(val_path)\n",
    "print(test_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
