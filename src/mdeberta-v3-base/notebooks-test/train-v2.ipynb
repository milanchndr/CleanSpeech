{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506f476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "!pip install -q --upgrade \\\n",
    "    transformers \\\n",
    "    tokenizers \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    accelerate \\\n",
    "    torchmetrics\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\n",
    "test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\")\n",
    "test_labels = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\")\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "train.head()\n",
    "\n",
    "\n",
    "\n",
    "# Count each toxic category\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Count how many 1s each column has\n",
    "category_counts = train[label_cols].sum().sort_values(ascending=False)\n",
    "print(\"âœ… Count of each label (how many comments have this label):\")\n",
    "print(category_counts)\n",
    "\n",
    "# Total number of positive labels across all columns\n",
    "total_positive_labels = category_counts.sum()\n",
    "print(\"\\nTotal number of label instances (sum of all 6 columns):\", total_positive_labels)\n",
    "\n",
    "# Number of comments that have at least one label\n",
    "rows_with_any_label = (train[label_cols].any(axis=1)).sum()\n",
    "print(\"Number of comments with at least one label:\", rows_with_any_label)\n",
    "\n",
    "# Number of comments with no label at all\n",
    "rows_with_no_label = len(train) - rows_with_any_label\n",
    "print(\"Number of comments with no label:\", rows_with_no_label)\n",
    "\n",
    "# (Optional) Distribution of how many labels each comment has (0â€“6)\n",
    "label_combo_counts = train[label_cols].sum(axis=1).value_counts().sort_index()\n",
    "print(\"\\nHow many labels each comment typically has (0â€“6):\")\n",
    "print(label_combo_counts)\n",
    "\n",
    "\n",
    "\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# 1) Show dtype of each label column\n",
    "print(train[label_cols].dtypes)\n",
    "\n",
    "# 2) Unique values per label (first 50 unique entries) â€” good to spot non-binary tokens\n",
    "for c in label_cols:\n",
    "    vals = train[c].dropna().astype(str).str.strip().unique()[:50]\n",
    "    print(f\"\\n{c} unique examples (up to 50):\\n\", vals)\n",
    "\n",
    "# 3) Count NaNs per label\n",
    "print(\"\\nNaN counts per label:\")\n",
    "print(train[label_cols].isna().sum())\n",
    "\n",
    "# 4) Count values other than '0' or '1' (string or numeric)\n",
    "for c in label_cols:\n",
    "    bad = train[~train[c].isin([0,1]) & ~train[c].isin(['0','1'])][c]\n",
    "    print(f\"{c} â€” non 0/1 sample count:\", len(bad))\n",
    "    if len(bad) > 0:\n",
    "        print(\"Example non-binary values:\", bad.astype(str).unique()[:20])\n",
    "\n",
    "# 5) Rows where labels sum > 6 or < 0 (impossible) â€” shows unexpected numeric values\n",
    "sums = train[label_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "print(\"\\nRows with label-sum < 0 or > 6 (should be none):\", ((sums<0) | (sums>6)).sum())\n",
    "\n",
    "# 6) Distribution of #labels per row (read-only)\n",
    "print(\"\\nDistribution (0..6) of number of positive labels per row:\")\n",
    "print(train[label_cols].apply(lambda s: pd.to_numeric(s, errors='coerce')).sum(axis=1).value_counts().sort_index())\n",
    "\n",
    "# 7) Duplicate checking by 'id' (if id exists) and by comment text\n",
    "if 'id' in train.columns:\n",
    "    print(\"\\nDuplicate id count:\", train['id'].duplicated().sum())\n",
    "    print(\"Unique duplicate id examples (up to 10):\", train[train['id'].duplicated(keep=False)]['id'].unique()[:10])\n",
    "if 'comment_text' in train.columns:\n",
    "    print(\"\\nDuplicate comment_text count:\", train['comment_text'].duplicated().sum())\n",
    "\n",
    "# 8) Show sample rows that are labeled but look suspicious (e.g., non-binary)\n",
    "mask_nonbinary = False\n",
    "for c in label_cols:\n",
    "    mask_nonbinary |= ~train[c].isin([0,1]) & ~train[c].isin(['0','1'])\n",
    "if mask_nonbinary.any():\n",
    "    print(\"\\nExample rows with non-binary label values (first 10):\")\n",
    "    print(train[mask_nonbinary].head(10)[['id','comment_text'] + label_cols])\n",
    "else:\n",
    "    print(\"\\nNo non-binary label values found in the subset checked.\")\n",
    "\n",
    "# 9) Check leading/trailing whitespace or invisible chars in label columns\n",
    "# Show rows where stringified label differs from stripped version\n",
    "def has_whitespace_issue(col):\n",
    "    s = train[col].astype(str)\n",
    "    return (s != s.str.strip()).any()\n",
    "for c in label_cols:\n",
    "    print(f\"{c} has leading/trailing whitespace?:\", has_whitespace_issue(c))\n",
    "\n",
    "\n",
    "\n",
    "    # Label columns\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "y = train[label_cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Define cleaning patterns (same as found during EDA)\n",
    "url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "html_pattern = r\"<.*?>\"\n",
    "emoji_pattern = r\"[\\U00010000-\\U0010ffff]\"       # captures emojis and symbols\n",
    "non_ascii_pattern = r\"[^\\x00-\\x7F]+\"             # captures non-ASCII chars\n",
    "multi_space_pattern = r\"\\s+\"                     # normalize spaces\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_pattern, \" \", text)\n",
    "    text = re.sub(html_pattern, \" \", text)\n",
    "    text = re.sub(emoji_pattern, \" \", text)\n",
    "    text = re.sub(non_ascii_pattern, \" \", text)\n",
    "    text = re.sub(multi_space_pattern, \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to train and test sets\n",
    "train[\"comment\"] = train[\"comment_text\"].apply(clean_text)\n",
    "test[\"comment\"] = test[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "# Quick preview to ensure it worked\n",
    "train[[\"comment_text\", \"comment\"]].head(5)\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "for label in label_cols:\n",
    "    sample_row = train[train[label] == 1].sample(1, random_state=random.randint(1,100))\n",
    "    print(f\"ðŸ”¹ Category: {label}\")\n",
    "    print(sample_row['comment_text'].values[0])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "\n",
    "    test_df = test.drop(columns=['comment_text','id'])\n",
    "test_df\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train = train.drop(columns=['id','comment_text' ])\n",
    "\n",
    "train\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Remove rows with missing or empty cleaned text\n",
    "before = len(train)\n",
    "train = train[train[\"comment\"].notna() & (train[\"comment\"].str.strip() != \"\")]\n",
    "after = len(train)\n",
    "print(f\"Removed {before - after} rows with missing or empty cleaned text.\")\n",
    "\n",
    "# 2. Remove duplicate comments (based on comment)\n",
    "before = len(train)\n",
    "train = train.drop_duplicates(subset=[\"comment\"])\n",
    "after = len(train)\n",
    "print(f\"Removed {before - after} duplicate rows.\")\n",
    "\n",
    "# 3. Trainâ€“Validation split (80â€“20), stratified on \"toxic\" label\n",
    "LABELS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[\"toxic\"] if \"toxic\" in train.columns else None\n",
    ")\n",
    "\n",
    "print(\"\\nFinal sizes:\")\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Validation:\", val_df.shape)\n",
    "\n",
    "# Optional: Print label distribution in train and val sets\n",
    "if all(col in train.columns for col in LABELS):\n",
    "    print(\"\\nTrain set label distribution:\")\n",
    "    print(train_df[LABELS].sum())\n",
    "    print(\"\\nValidation set label distribution:\")\n",
    "    print(val_df[LABELS].sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ====================================================\n",
    "# KAGGLE MULTI-GPU SETUP\n",
    "# ====================================================\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW  # âœ… Import from torch.optim instead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# ====================================================\n",
    "# CONFIG\n",
    "# ====================================================\n",
    "MODEL_PATH = \"microsoft/mdeberta-v3-base\"\n",
    "NUM_LABELS = 6\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 16  # Per GPU, effective = 32\n",
    "LR = 2e-5\n",
    "MAX_LEN = 256\n",
    "SEED = 42\n",
    "PATIENCE = 2\n",
    "MIN_DELTA = 0.0005\n",
    "\n",
    "LABELS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ====================================================\n",
    "# CHECK GPU AVAILABILITY\n",
    "# ====================================================\n",
    "print(f\"ðŸ”§ GPUs available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# ====================================================\n",
    "# TOKENIZER\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# ====================================================\n",
    "# DATASET CLASS\n",
    "# ====================================================\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256, is_test=False):\n",
    "        self.texts = df[\"comment\"].tolist()\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        if not is_test:\n",
    "            self.labels = df[LABELS].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        if not self.is_test:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# CREATE DATASETS\n",
    "# ====================================================\n",
    "train_dataset = ToxicDataset(train_df, tokenizer, max_len=MAX_LEN)\n",
    "val_dataset = ToxicDataset(val_df, tokenizer, max_len=MAX_LEN)\n",
    "test_dataset = ToxicDataset(test_df, tokenizer, max_len=MAX_LEN, is_test=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# WEIGHTED LOSS\n",
    "# ====================================================\n",
    "label_sum = train_df[LABELS].sum(axis=0)\n",
    "label_freq = label_sum / len(train_df)\n",
    "class_weights = (1 / (label_freq + 1e-6))\n",
    "class_weights = class_weights / class_weights.sum() * NUM_LABELS\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float)\n",
    "\n",
    "print(\"âœ… Class Weights:\", class_weights_tensor)\n",
    "\n",
    "class WeightedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, targets, weight=self.weights.to(logits.device)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "criterion = WeightedBCEWithLogitsLoss(class_weights_tensor)\n",
    "\n",
    "# ====================================================\n",
    "# OPTIMIZER & SCHEDULER\n",
    "# ====================================================\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)  # âœ… Now from torch.optim\n",
    "num_training_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# âœ… ACCELERATOR - AUTOMATICALLY USES BOTH GPUs\n",
    "# ====================================================\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\",\n",
    "    gradient_accumulation_steps=1\n",
    ")\n",
    "\n",
    "# Prepare everything\n",
    "train_loader, val_loader, test_loader, model, optimizer, scheduler = accelerator.prepare(\n",
    "    train_loader, val_loader, test_loader, model, optimizer, scheduler\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training on {accelerator.num_processes} GPU(s)\")\n",
    "print(f\"âœ… Effective batch size: {BATCH_SIZE * accelerator.num_processes}\")\n",
    "\n",
    "# ====================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# ====================================================\n",
    "best_auc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_path = \"best_model\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    else:\n",
    "        progress = train_loader\n",
    "\n",
    "    for batch in progress:\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, batch[\"labels\"])\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if accelerator.is_main_process:\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # ====================================================\n",
    "    # VALIDATION\n",
    "    # ====================================================\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = criterion(logits, batch[\"labels\"])\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_probs = accelerator.gather(probs)\n",
    "            all_labels = accelerator.gather(batch[\"labels\"])\n",
    "            \n",
    "            preds.append(all_probs.cpu().numpy())\n",
    "            refs.append(all_labels.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    refs = np.concatenate(refs)\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        preds = preds[:len(val_dataset)]\n",
    "        refs = refs[:len(val_dataset)]\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        valid_aucs = []\n",
    "        print(\"\\nPer-class AUC:\")\n",
    "        for i, label in enumerate(LABELS):\n",
    "            if len(np.unique(refs[:, i])) > 1:\n",
    "                auc_score = roc_auc_score(refs[:, i], preds[:, i])\n",
    "                valid_aucs.append(auc_score)\n",
    "                print(f\"  {label:15s}: {auc_score:.4f}\")\n",
    "        \n",
    "        macro_auc = np.mean(valid_aucs) if valid_aucs else 0.0\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f} | Val AUC (macro): {macro_auc:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if macro_auc > best_auc + MIN_DELTA:\n",
    "            best_auc = macro_auc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                best_model_path,\n",
    "                save_function=accelerator.save\n",
    "            )\n",
    "            tokenizer.save_pretrained(best_model_path)\n",
    "            print(f\"âœ… Best model saved! AUC: {best_auc:.4f}\\n\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"âš ï¸  No improvement. Patience: {patience_counter}/{PATIENCE}\\n\")\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"ðŸ›‘ Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… Training Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Validation AUC: {best_auc:.4f}\")\n",
    "    print(f\"Model saved at: {best_model_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ====================================================\n",
    "# TEST INFERENCE\n",
    "# ====================================================\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        outputs = model(**batch)\n",
    "        logits = torch.sigmoid(outputs.logits)\n",
    "        test_preds.append(accelerator.gather(logits).detach().cpu().numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds)\n",
    "submission = pd.DataFrame(test_preds, columns=label_cols)\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… submission.csv saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
