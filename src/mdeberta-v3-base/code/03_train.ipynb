{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d204557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "CUDA available: False\n",
      "Running on CPU\n",
      "../../data/train_data.csv\n",
      "microsoft/mdeberta-v3-base\n",
      "âœ… Config loaded and random seed set to: 42\n",
      "ðŸ“‚ Model directory: ../models/best\n",
      "ðŸ“‚ Reports directory: ../reports\n",
      "âœ… Folder setup complete.\n",
      "âœ… Found: ..\\..\\data\\train_data.csv\n",
      "âœ… Found: ..\\..\\data\\test_data.csv\n",
      "\n",
      "All required data files are present and accessible.\n",
      "âœ… Configuration snapshot saved at:\n",
      "../reports\\config_snapshot.json\n",
      "âœ… ToxicDataset ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” Setup + Dataset class\n",
    "%run ./00_config.ipynb\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Mirror of train.py's ToxicDataset\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, is_test=False):\n",
    "        self.texts = df[\"comment\"].tolist()\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        # pick only the labels present in df, in cfg.labels order\n",
    "        if not is_test:\n",
    "            keep = [c for c in cfg.labels if c in df.columns]\n",
    "            self.labels = df[keep].values.astype(\"float32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=cfg.train.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if not self.is_test:\n",
    "            item[\"labels\"] = torch.from_numpy(self.labels[i])\n",
    "        return item\n",
    "\n",
    "print(\"âœ… ToxicDataset ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46228283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” Helpers\n",
    "\n",
    "import os, json\n",
    "\n",
    "def _ensure_dirs(report_dir, model_dir):\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(report_dir, \"figs\"), exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "def _plot_curves(report_dir, train_losses, val_aucs):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Train loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Train Loss\"); plt.title(\"Train Loss\"); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(report_dir, \"figs\", \"train_loss.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Val macro AUC\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(val_aucs) + 1), val_aucs, marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Macro AUC\"); plt.title(\"Validation Macro AUC\"); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(report_dir, \"figs\", \"val_macro_auc.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def _append_run_summary(report_dir, row: dict):\n",
    "    import csv, datetime, hashlib\n",
    "    path = os.path.join(report_dir, \"run_summary.csv\")\n",
    "    row = row.copy()\n",
    "    row[\"timestamp\"] = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "    conf_str = json.dumps(row.get(\"config_snapshot\", {}), sort_keys=True)\n",
    "    row[\"config_hash\"] = hashlib.md5(conf_str.encode()).hexdigest()[:8]\n",
    "    row.pop(\"config_snapshot\", None)\n",
    "    header = [\"timestamp\",\"run_id\",\"config_hash\",\"model_name\",\"max_len\",\"batch_size\",\"lr\",\n",
    "              \"epochs\",\"patience\",\"macro_auc\",\"best_epoch\",\"model_dir\"]\n",
    "    exists = os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        if not exists:\n",
    "            w.writeheader()\n",
    "        w.writerow({k: row.get(k, \"\") for k in header})\n",
    "\n",
    "print(\"âœ… Helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bc7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding splits quickly (01_data not in memory).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete | device processes: 1\n",
      "Train batches/epoch: 7979 | Val batches/epoch: 1995\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” Wire up loaders, model, loss, optimizer, scheduler, accelerator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 0) Ensure we have train_df/val_df; fallback to quick rebuild if this kernel is fresh\n",
    "try:\n",
    "    _ = train_df, val_df\n",
    "    print(\"Using train/val already in memory from 01_data.ipynb.\")\n",
    "except NameError:\n",
    "    print(\"Rebuilding splits quickly (01_data not in memory).\")\n",
    "    # minimal cleaner (same as 01_data)\n",
    "    _HTML = re.compile(r\"<.*?>\"); _URL = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "    _NONASCII = re.compile(r\"[^\\x00-\\x7F]+\"); _WS = re.compile(r\"\\s+\")\n",
    "    def clean_text(t: str) -> str:\n",
    "        if not isinstance(t, str): return \"\"\n",
    "        t = t.lower(); t = _URL.sub(\" \", t); t = _HTML.sub(\" \", t); t = _NONASCII.sub(\" \", t); t = _WS.sub(\" \", t).strip()\n",
    "        return t\n",
    "\n",
    "    raw = pd.read_csv(cfg.paths.raw_train)\n",
    "    text_col = \"comment_text\" if \"comment_text\" in raw.columns else (\"comment\" if \"comment\" in raw.columns else None)\n",
    "    assert text_col is not None, \"Expected 'comment_text' or 'comment' in train CSV.\"\n",
    "    raw[\"comment\"] = raw[text_col].apply(clean_text)\n",
    "    label_cols = [c for c in cfg.labels if c in raw.columns]\n",
    "    strat = raw[\"toxic\"] if \"toxic\" in raw.columns else None\n",
    "    train_df, val_df = train_test_split(raw, test_size=0.2, random_state=cfg.train.seed, stratify=strat)\n",
    "    keep = [\"comment\"] + label_cols\n",
    "    train_df = train_df[keep].reset_index(drop=True)\n",
    "    val_df   = val_df[keep].reset_index(drop=True)\n",
    "\n",
    "label_cols = [c for c in cfg.labels if c in train_df.columns]\n",
    "assert len(label_cols) == len(cfg.labels), f\"Expected all labels {cfg.labels}, found {label_cols}\"\n",
    "\n",
    "# 1) Tokenizer & collator\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.train.model_name)\n",
    "collate = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 2) Datasets & loaders (reuse ToxicDataset from Cell 1)\n",
    "tset = ToxicDataset(train_df, tokenizer, cfg.train.max_len)\n",
    "vset = ToxicDataset(val_df, tokenizer, cfg.train.max_len)\n",
    "\n",
    "tl = DataLoader(tset, batch_size=cvfg.train.batch_size if 'cvfg' in globals() else cfg.train.batch_size,\n",
    "                shuffle=True, num_workers=2, collate_fn=collate, pin_memory=True)\n",
    "vl = DataLoader(vset, batch_size=cfg.train.batch_size, shuffle=False, num_workers=2, collate_fn=collate, pin_memory=True)\n",
    "\n",
    "# 3) Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.train.model_name,\n",
    "    num_labels=len(cfg.labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# 4) Weighted BCE loss (compute label frequencies from train_df)\n",
    "freq = (train_df[label_cols].sum(axis=0).values / len(train_df))\n",
    "weights = 1.0 / (freq + 1e-6); weights = weights / weights.sum() * len(freq)\n",
    "weights_t = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        logits, targets, weight=weights_t.to(logits.device)\n",
    "    )\n",
    "\n",
    "# 5) Optimizer & scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "total_steps = len(tl) * cfg.train.epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, int(cfg.train.warmup_ratio * total_steps), total_steps)\n",
    "\n",
    "# 6) Accelerator\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "tl, vl, model, optimizer, scheduler = accelerator.prepare(tl, vl, model, optimizer, scheduler)\n",
    "\n",
    "print(f\"âœ… Setup complete | device processes: {accelerator.num_processes}\")\n",
    "print(f\"Train batches/epoch: {len(tl)} | Val batches/epoch: {len(vl)}\")\n",
    "print(f\"Effective batch size: {cfg.train.batch_size * accelerator.num_processes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# full tune\n",
    "# Cell 4 â€” Train + Validate + Save best\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Ensure output dirs exist\n",
    "report_dir = cfg.paths.reports_dir\n",
    "model_dir  = cfg.paths.model_dir\n",
    "_ensure_dirs(report_dir, model_dir)\n",
    "\n",
    "best_auc, best_epoch, patience_ctr = 0.0, -1, 0\n",
    "train_losses, val_macro_aucs = [], []\n",
    "run_id = f\"run_{int(time.time())}\"\n",
    "MIN_DELTA = 5e-4\n",
    "\n",
    "for epoch in range(cfg.train.epochs):\n",
    "    # ---- Train\n",
    "    model.train()\n",
    "    ep_loss = 0.0\n",
    "    for batch in tl:\n",
    "        outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"})\n",
    "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "        accelerator.backward(loss)\n",
    "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "        ep_loss += loss.item()\n",
    "    ep_loss /= max(1, len(tl))\n",
    "    train_losses.append(ep_loss)\n",
    "\n",
    "    # ---- Validate\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in vl:\n",
    "            outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"})\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            preds.append(accelerator.gather(probs).cpu().numpy())\n",
    "            refs.append(accelerator.gather(batch[\"labels\"]).cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)[:len(vset)]\n",
    "    refs  = np.concatenate(refs)[:len(vset)]\n",
    "\n",
    "    # Per-label AUC (skip labels missing in this fold)\n",
    "    per_label_aucs = []\n",
    "    for i, lab in enumerate(cfg.labels):\n",
    "        y_true = refs[:, i]\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            continue\n",
    "        per_label_aucs.append(roc_auc_score(y_true, preds[:, i]))\n",
    "    macro_auc = float(np.mean(per_label_aucs)) if per_label_aucs else 0.0\n",
    "    val_macro_aucs.append(macro_auc)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Epoch {epoch+1}/{cfg.train.epochs} | train_loss={ep_loss:.4f} | val_macro_auc={macro_auc:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if macro_auc > best_auc + MIN_DELTA:\n",
    "            best_auc, best_epoch = macro_auc, epoch + 1\n",
    "            patience_ctr = 0\n",
    "            unwrapped = accelerator.unwrap_model(model)\n",
    "            unwrapped.save_pretrained(model_dir, save_function=accelerator.save)\n",
    "            tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "            with open(os.path.join(report_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"macro_auc\": best_auc,\n",
    "                    \"per_label_auc\": {str(i): float(a) for i, a in enumerate(per_label_aucs)},\n",
    "                    \"best_epoch\": best_epoch\n",
    "                }, f, indent=2)\n",
    "\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "\n",
    "        # Update curves each epoch\n",
    "        _plot_curves(report_dir, train_losses, val_macro_aucs)\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_ctr >= cfg.train.patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "# Append run summary (main process only)\n",
    "if accelerator.is_main_process:\n",
    "    from dataclasses import asdict\n",
    "    _append_run_summary(report_dir, {\n",
    "        \"run_id\": run_id,\n",
    "        \"model_name\": cfg.train.model_name,\n",
    "        \"max_len\": cfg.train.max_len,\n",
    "        \"batch_size\": cfg.train.batch_size,\n",
    "        \"lr\": cfg.train.lr,\n",
    "        \"epochs\": cfg.train.epochs,\n",
    "        \"patience\": cfg.train.patience,\n",
    "        \"macro_auc\": best_auc,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"model_dir\": model_dir,\n",
    "        \"config_snapshot\": {\n",
    "            \"paths\": asdict(cfg.paths),\n",
    "            \"train\": asdict(cfg.train),\n",
    "            \"labels\": list(cfg.labels)\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… Training complete.\")\n",
    "print(f\"Best macro AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Curves saved under: {os.path.join(report_dir, 'figs')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1700e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaud\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing existing Accelerator with mixed_precision: fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [36:18<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | train_loss=0.0870 | val_macro_auc=0.8034\n",
      "\n",
      "âœ… Demo training complete.\n",
      "Model saved to: ../models/best\n",
      "Curves in: ../reports\\figs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alaud\\AppData\\Local\\Temp\\ipykernel_15236\\2869582185.py:31: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  row[\"timestamp\"] = datetime.datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "# Demo training: small subset + short seq length + 1 epoch + progress bar\n",
    "import os, json, time, numpy as np, torch, pandas as pd, re\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# ---------- demo knobs ----------\n",
    "DEMO_TRAIN_ROWS = 4000\n",
    "DEMO_VAL_ROWS   = 1000\n",
    "DEMO_EPOCHS     = 1\n",
    "DEMO_MAX_LEN    = 128\n",
    "BATCH_SIZE      = 8\n",
    "MIN_DELTA       = 5e-4\n",
    "\n",
    "# ---------- rebuild a minimal split for speed ----------\n",
    "_HTML = re.compile(r\"<.*?>\"); _URL = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_NONASCII = re.compile(r\"[^\\x00-\\x7F]+\"); _WS = re.compile(r\"\\s+\")\n",
    "def clean_text(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    t = t.lower(); t = _URL.sub(\" \", t); t = _HTML.sub(\" \", t); t = _NONASCII.sub(\" \", t); t = _WS.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "raw = pd.read_csv(cfg.paths.raw_train)\n",
    "text_col = \"comment_text\" if \"comment_text\" in raw.columns else (\"comment\" if \"comment\" in raw.columns else None)\n",
    "assert text_col is not None, \"Need 'comment_text' or 'comment' in train_data.csv\"\n",
    "raw[\"comment\"] = raw[text_col].apply(clean_text)\n",
    "label_cols = [c for c in cfg.labels if c in raw.columns]\n",
    "# stratify on toxic if present\n",
    "strat = raw[\"toxic\"] if \"toxic\" in raw.columns else None\n",
    "# take a small random subset for speed\n",
    "raw_small = (raw.sample(DEMO_TRAIN_ROWS + DEMO_VAL_ROWS, random_state=cfg.train.seed)\n",
    "                  if len(raw) > DEMO_TRAIN_ROWS + DEMO_VAL_ROWS else raw.copy())\n",
    "train_df = raw_small.iloc[:DEMO_TRAIN_ROWS][[\"comment\"] + label_cols].reset_index(drop=True)\n",
    "val_df   = raw_small.iloc[DEMO_TRAIN_ROWS:DEMO_TRAIN_ROWS+DEMO_VAL_ROWS][[\"comment\"] + label_cols].reset_index(drop=True)\n",
    "\n",
    "# ---------- tokenizer, datasets, loaders ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.train.model_name)\n",
    "collate = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "class ToxicDatasetDemo(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tok, max_len):\n",
    "        self.texts = df[\"comment\"].tolist()\n",
    "        self.labels = df[label_cols].values.astype(\"float32\")\n",
    "        self.tok, self.max_len = tok, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\", max_length=DEMO_MAX_LEN, return_tensors=\"pt\")\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.from_numpy(self.labels[i])\n",
    "        return item\n",
    "\n",
    "tset = ToxicDatasetDemo(train_df, tokenizer, DEMO_MAX_LEN)\n",
    "vset = ToxicDatasetDemo(val_df, tokenizer, DEMO_MAX_LEN)\n",
    "\n",
    "tl = DataLoader(tset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate)\n",
    "vl = DataLoader(vset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate)\n",
    "\n",
    "# ---------- model, loss, optim, sched ----------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.train.model_name, num_labels=len(cfg.labels), problem_type=\"multi_label_classification\"\n",
    ")\n",
    "freq = (train_df[label_cols].sum(axis=0).values / len(train_df))\n",
    "weights = 1.0 / (freq + 1e-6); weights = weights / weights.sum() * len(freq)\n",
    "w_t = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    return torch.nn.functional.binary_cross_entropy_with_logits(logits, targets, weight=w_t.to(logits.device))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "total_steps = len(tl) * DEMO_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, int(cfg.train.warmup_ratio * total_steps), total_steps)\n",
    "\n",
    "# CPU-friendly accelerator config\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Reuse existing Accelerator if present; otherwise create one that matches device\n",
    "try:\n",
    "    accelerator  # already exists\n",
    "    print(\"Reusing existing Accelerator with mixed_precision:\",\n",
    "          getattr(getattr(accelerator, \"state\", None), \"mixed_precision\", \"unknown\"))\n",
    "except NameError:\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\" if torch.cuda.is_available() else \"no\")\n",
    "\n",
    "# Prepare using the (reused or newly created) accelerator\n",
    "tl, vl, model, optimizer, scheduler = accelerator.prepare(tl, vl, model, optimizer, scheduler)\n",
    "\n",
    "# ---------- train one fast epoch with progress bar ----------\n",
    "report_dir, model_dir = cfg.paths.reports_dir, cfg.paths.model_dir\n",
    "_ensure_dirs(report_dir, model_dir)\n",
    "best_auc, best_epoch, patience_ctr = 0.0, -1, 0\n",
    "train_losses, val_macro_aucs = [], []\n",
    "run_id = f\"demo_{int(time.time())}\"\n",
    "\n",
    "for epoch in range(DEMO_EPOCHS):\n",
    "    model.train()\n",
    "    ep_loss = 0.0\n",
    "    for batch in tqdm(tl, desc=f\"Epoch {epoch+1}/{DEMO_EPOCHS}\"):\n",
    "        outputs = model(**{k:v for k,v in batch.items() if k!=\"labels\"})\n",
    "        loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "        ep_loss += loss.item()\n",
    "    ep_loss /= max(1, len(tl))\n",
    "    train_losses.append(ep_loss)\n",
    "\n",
    "    # validate\n",
    "    model.eval(); preds, refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in vl:\n",
    "            outputs = model(**{k:v for k,v in batch.items() if k!=\"labels\"})\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            preds.append(accelerator.gather(probs).cpu().numpy())\n",
    "            refs.append(accelerator.gather(batch[\"labels\"]).cpu().numpy())\n",
    "    preds = np.concatenate(preds)[:len(vset)]\n",
    "    refs  = np.concatenate(refs)[:len(vset)]\n",
    "\n",
    "    per_label_aucs = []\n",
    "    for i, lab in enumerate(cfg.labels):\n",
    "        y_true = refs[:, i]\n",
    "        if len(np.unique(y_true)) < 2: continue\n",
    "        per_label_aucs.append(roc_auc_score(y_true, preds[:, i]))\n",
    "    macro_auc = float(np.mean(per_label_aucs)) if per_label_aucs else 0.0\n",
    "    val_macro_aucs.append(macro_auc)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Epoch {epoch+1}/{DEMO_EPOCHS} | train_loss={ep_loss:.4f} | val_macro_auc={macro_auc:.4f}\")\n",
    "\n",
    "        # save checkpoint (best = only epoch here)\n",
    "        unwrapped = accelerator.unwrap_model(model)\n",
    "        unwrapped.save_pretrained(model_dir, save_function=accelerator.save)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "        with open(os.path.join(report_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"macro_auc\": macro_auc, \"per_label_auc\": {str(i): float(a) for i,a in enumerate(per_label_aucs)}, \"best_epoch\": epoch+1}, f, indent=2)\n",
    "\n",
    "        _plot_curves(report_dir, train_losses, val_macro_aucs)\n",
    "\n",
    "# Append a short demo run summary\n",
    "if accelerator.is_main_process:\n",
    "    from dataclasses import asdict\n",
    "    _append_run_summary(report_dir, {\n",
    "        \"run_id\": run_id,\n",
    "        \"model_name\": cfg.train.model_name,\n",
    "        \"max_len\": DEMO_MAX_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": cfg.train.lr,\n",
    "        \"epochs\": DEMO_EPOCHS,\n",
    "        \"patience\": cfg.train.patience,\n",
    "        \"macro_auc\": val_macro_aucs[-1] if val_macro_aucs else 0.0,\n",
    "        \"best_epoch\": 1,\n",
    "        \"model_dir\": model_dir,\n",
    "        \"config_snapshot\": {\"paths\": asdict(cfg.paths), \"train\": asdict(cfg.train), \"labels\": list(cfg.labels)}\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… Demo training complete.\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Curves in: {os.path.join(report_dir, 'figs')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a2b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: ../models/best\n",
      "Reports : ../reports\n",
      "\n",
      "Contents of model dir:\n",
      "['added_tokens.json', 'config.json', 'model.safetensors', 'special_tokens_map.json', 'spm.model', 'tokenizer.json', 'tokenizer_config.json']\n",
      "\n",
      "metrics.json exists: True\n",
      "metrics: {'macro_auc': 0.8034086988331047, 'best_epoch': 1}\n"
     ]
    }
   ],
   "source": [
    "# Verify artifacts from demo training\n",
    "import os, json\n",
    "\n",
    "print(\"Model dir:\", cfg.paths.model_dir)\n",
    "print(\"Reports :\", cfg.paths.reports_dir)\n",
    "\n",
    "print(\"\\nContents of model dir:\")\n",
    "print(os.listdir(cfg.paths.model_dir) if os.path.exists(cfg.paths.model_dir) else \"!! not found\")\n",
    "\n",
    "metrics_path = os.path.join(cfg.paths.reports_dir, \"metrics.json\")\n",
    "print(\"\\nmetrics.json exists:\", os.path.exists(metrics_path))\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = json.load(f)\n",
    "    print(\"metrics:\", {k: m.get(k) for k in [\"macro_auc\", \"best_epoch\"]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
