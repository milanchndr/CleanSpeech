{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7411d7a",
   "metadata": {},
   "source": [
    "# 1. DATA INSPECTION AND QUALITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f818f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (159571, 8)\n",
      "\n",
      "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "\n",
      "Data types:\n",
      " id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "Empty text entries: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Inspection & Quality Check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from the data folder\n",
    "df = pd.read_csv(\"data/train_data.csv\")\n",
    "\n",
    "# Basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Empty text values (non-null but blank)\n",
    "empty_text_count = df['comment_text'].apply(lambda x: str(x).strip() == '').sum()\n",
    "print(f\"Empty text entries: {empty_text_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d422895",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The dataset is **clean and well-structured**.  \n",
    "- There are **no missing, duplicate, or empty entries**, indicating reliable data quality.  \n",
    "- Each record includes an `id`, the user’s comment, and six binary toxicity labels.  \n",
    "- The dataset is ready for exploratory and statistical analysis.\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b18fa3",
   "metadata": {},
   "source": [
    "# 2. LABEL DISTRIBUTION AND CLASS BALANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15527d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      " toxic            15294\n",
      "obscene           8449\n",
      "insult            7877\n",
      "severe_toxic      1595\n",
      "identity_hate     1405\n",
      "threat             478\n",
      "dtype: int64\n",
      "\n",
      "Non-toxic comments: 143346 (89.83%)\n",
      "Toxic comments: 16225 (10.17%)\n"
     ]
    }
   ],
   "source": [
    "# 2. Label Distribution & Class Balance Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only label columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Count total toxic labels per column\n",
    "label_counts = df[label_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Label counts:\\n\", label_counts)\n",
    "\n",
    "\n",
    "# Calculate percentage of non-toxic vs toxic comments\n",
    "non_toxic = (df[label_cols].sum(axis=1) == 0).sum()\n",
    "toxic = df.shape[0] - non_toxic\n",
    "print(f\"\\nNon-toxic comments: {non_toxic} ({non_toxic/df.shape[0]*100:.2f}%)\")\n",
    "print(f\"Toxic comments: {toxic} ({toxic/df.shape[0]*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86707c",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The dataset is **highly imbalanced**, with almost **90% non-toxic comments**.  \n",
    "- Among toxic categories, the **\"toxic\"** label is most common, followed by *obscene* and *insult*.  \n",
    "- **Severe_toxic**, **identity_hate**, and **threat** are rare categories, each below 1%.  \n",
    "- The class imbalance indicates that later stages may need **sampling strategies** or **weighted loss functions** to prevent the model from being biased toward non-toxic examples.\n",
    "\n",
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820d44f",
   "metadata": {},
   "source": [
    "# 3. COMMENT LENGTH AND TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5245b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character length summary:\n",
      " count    159571.000000\n",
      "mean        394.073221\n",
      "std         590.720282\n",
      "min           6.000000\n",
      "25%          96.000000\n",
      "50%         205.000000\n",
      "75%         435.000000\n",
      "max        5000.000000\n",
      "Name: char_length, dtype: float64\n",
      "\n",
      "Word count summary:\n",
      " count    159571.000000\n",
      "mean         67.273527\n",
      "std          99.230702\n",
      "min           1.000000\n",
      "25%          17.000000\n",
      "50%          36.000000\n",
      "75%          75.000000\n",
      "max        1411.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Average word count - Toxic: 51.29\n",
      "Average word count - Non-toxic: 68.97\n"
     ]
    }
   ],
   "source": [
    "# 3. Comment Length & Text Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute lengths\n",
    "df[\"char_length\"] = df[\"comment_text\"].apply(len)\n",
    "df[\"word_count\"] = df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Character length summary:\\n\", df[\"char_length\"].describe())\n",
    "print(\"\\nWord count summary:\\n\", df[\"word_count\"].describe())\n",
    "\n",
    "# Compare average length by toxicity\n",
    "toxic_comments = df[df[\"toxic\"] == 1]\n",
    "non_toxic_comments = df[df[\"toxic\"] == 0]\n",
    "\n",
    "avg_len_toxic = toxic_comments[\"word_count\"].mean()\n",
    "avg_len_nontoxic = non_toxic_comments[\"word_count\"].mean()\n",
    "\n",
    "print(f\"\\nAverage word count - Toxic: {avg_len_toxic:.2f}\")\n",
    "print(f\"Average word count - Non-toxic: {avg_len_nontoxic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321adec",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The average comment length is around **67 words** (≈ 394 characters).  \n",
    "- The distribution is **right-skewed**, meaning most comments are short, with a few long ones up to 1400+ words.  \n",
    "- Interestingly, **non-toxic comments are longer on average** than toxic ones, suggesting that toxic messages are often shorter and more direct.  \n",
    "- No extreme anomalies were detected — the length statistics align with typical online comment data.\n",
    "\n",
    "--------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb736f0",
   "metadata": {},
   "source": [
    "# 4. WORD-LEVEL EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02ee5445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words in TOXIC comments:\n",
      "you             35329\n",
      "the             20451\n",
      "a               20373\n",
      "i               19902\n",
      "to              15655\n",
      "and             15647\n",
      "is              12630\n",
      "of              11386\n",
      "your            9114\n",
      "fuck            8617\n",
      "are             8153\n",
      "that            7736\n",
      "it              6831\n",
      "in              6663\n",
      "my              6170\n",
      "this            5432\n",
      "me              5390\n",
      "on              4858\n",
      "not             4809\n",
      "for             4705\n",
      "\n",
      "Top 20 words in NON-TOXIC comments:\n",
      "the             475029\n",
      "to              281196\n",
      "of              212638\n",
      "and             206721\n",
      "a               194537\n",
      "i               180753\n",
      "you             169231\n",
      "is              163330\n",
      "that            146565\n",
      "in              137525\n",
      "it              122812\n",
      "for             97740\n",
      "this            91649\n",
      "not             88529\n",
      "on              84589\n",
      "be              79756\n",
      "as              74235\n",
      "have            67564\n",
      "are             63723\n",
      "if              55660\n"
     ]
    }
   ],
   "source": [
    "# 4. Word-Level Exploration (No Visuals)\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning: lowercase, remove punctuation and numbers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply basic cleaning\n",
    "df['clean_text'] = df['comment_text'].apply(clean_text)\n",
    "\n",
    "# Separate toxic and non-toxic comments\n",
    "toxic_texts = \" \".join(df[df[\"toxic\"] == 1][\"clean_text\"])\n",
    "nontoxic_texts = \" \".join(df[df[\"toxic\"] == 0][\"clean_text\"])\n",
    "\n",
    "# Count top words\n",
    "toxic_words = Counter(toxic_texts.split()).most_common(20)\n",
    "nontoxic_words = Counter(nontoxic_texts.split()).most_common(20)\n",
    "\n",
    "print(\"Top 20 words in TOXIC comments:\")\n",
    "for word, count in toxic_words:\n",
    "    print(f\"{word:<15} {count}\")\n",
    "\n",
    "print(\"\\nTop 20 words in NON-TOXIC comments:\")\n",
    "for word, count in nontoxic_words:\n",
    "    print(f\"{word:<15} {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b33ed3",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Both toxic and non-toxic comments contain common English stopwords (e.g., *the*, *to*, *and*), which is typical for natural text.  \n",
    "- The toxic comments, however, include **aggressive or explicit vocabulary** such as *“fuck”* and possessive tones (*your*, *you*, *my*).  \n",
    "- This indicates that toxic messages often use **direct, confrontational language** targeting individuals.  \n",
    "- Removing stopwords and analyzing remaining tokens later will help isolate **discriminative words** for model training and interpretability.\n",
    "\n",
    "-------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa72fe",
   "metadata": {},
   "source": [
    "# 5. LABEL CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b621ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Correlation Matrix:\n",
      "\n",
      "               toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "toxic          1.000         0.309    0.677   0.157   0.648          0.266\n",
      "severe_toxic   0.309         1.000    0.403   0.124   0.376          0.202\n",
      "obscene        0.677         0.403    1.000   0.141   0.741          0.287\n",
      "threat         0.157         0.124    0.141   1.000   0.150          0.115\n",
      "insult         0.648         0.376    0.741   0.150   1.000          0.338\n",
      "identity_hate  0.266         0.202    0.287   0.115   0.338          1.000\n"
     ]
    }
   ],
   "source": [
    "# 5. Label Correlation Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select label columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df[label_cols].corr()\n",
    "\n",
    "# Display numerical correlation\n",
    "print(\"Label Correlation Matrix:\\n\")\n",
    "print(corr_matrix.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634504",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The **strongest correlations** are between:\n",
    "  - `obscene` and `insult` (**0.74**)  \n",
    "  - `toxic` and `obscene` (**0.68**)  \n",
    "  - `toxic` and `insult` (**0.65**)  \n",
    "- These relationships indicate that many comments marked as *obscene* or *insult* are also labeled *toxic*.  \n",
    "- `severe_toxic` shows a **moderate correlation** with `toxic` (**0.31**) — expected, as it’s a more extreme subset.  \n",
    "- `threat` has **weak correlations** with all other labels, confirming it represents a distinct minority category.  \n",
    "- `identity_hate` also has relatively low correlations, suggesting it captures a unique type of targeted toxicity.\n",
    "\n",
    "**Inference**\n",
    "\n",
    "The labels are **not independent**, which supports treating this as a **multi-label classification problem** rather than multiple single-label tasks.  \n",
    "This correlation insight can also guide **loss weighting**, **label grouping**, or **hierarchical modeling** in later stages.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e9d12",
   "metadata": {},
   "source": [
    "# 6. DATA QUALITY AND CLEANING CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44771080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Cleaning Indicators:\n",
      "\n",
      "has_url           5104\n",
      "has_html           129\n",
      "has_emoji           98\n",
      "has_non_ascii    17215\n",
      "dtype: int64\n",
      "\n",
      "Proportion of affected rows:\n",
      "has_url           3.199\n",
      "has_html          0.081\n",
      "has_emoji         0.061\n",
      "has_non_ascii    10.788\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Data Quality & Cleaning Checks\n",
    "\n",
    "import re\n",
    "\n",
    "# Helper regex patterns\n",
    "url_pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
    "html_pattern = r'<.*?>'\n",
    "emoji_pattern = r'[\\U00010000-\\U0010ffff]'\n",
    "non_ascii_pattern = r'[^\\x00-\\x7F]+'\n",
    "\n",
    "# Check for presence of patterns\n",
    "df['has_url'] = df['comment_text'].str.contains(url_pattern, regex=True)\n",
    "df['has_html'] = df['comment_text'].str.contains(html_pattern, regex=True)\n",
    "df['has_emoji'] = df['comment_text'].str.contains(emoji_pattern, regex=True)\n",
    "df['has_non_ascii'] = df['comment_text'].str.contains(non_ascii_pattern, regex=True)\n",
    "\n",
    "# Summary\n",
    "print(\"Text Cleaning Indicators:\\n\")\n",
    "print(df[['has_url', 'has_html', 'has_emoji', 'has_non_ascii']].sum())\n",
    "\n",
    "# Calculate proportions\n",
    "total_rows = df.shape[0]\n",
    "print(\"\\nProportion of affected rows:\")\n",
    "print((df[['has_url', 'has_html', 'has_emoji', 'has_non_ascii']].sum() / total_rows * 100).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd71f0",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- A small number of comments (**~3.2%**) contain embedded **URLs**, likely referencing web content or sources.  \n",
    "- **HTML tags** and **emojis** are rare, appearing in less than 0.1% of the data.  \n",
    "- Around **10.8%** of comments contain **non-ASCII characters**, indicating possible foreign language text, special symbols, or encoding artifacts.  \n",
    "- Overall, text noise is **minimal**, but basic preprocessing (URL and non-ASCII removal or normalization) will improve model consistency.\n",
    "\n",
    "**Inference**\n",
    "\n",
    "The dataset is generally clean but would benefit from:\n",
    "- Removing URLs and HTML tags.  \n",
    "- Normalizing or stripping non-ASCII text where appropriate.  \n",
    "- Keeping emoji information optional (if sentiment cues are relevant).  \n",
    "\n",
    "This ensures consistent text formatting for tokenization and feature extraction during model training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
